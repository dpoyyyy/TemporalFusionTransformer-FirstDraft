{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install jdatetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q-wff-WIuSF",
        "outputId": "ff784ec6-0212-4299-ba2d-d980a2db3d5b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jdatetime\n",
            "  Downloading jdatetime-5.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jalali-core>=1.0 (from jdatetime)\n",
            "  Downloading jalali_core-1.0.0-py3-none-any.whl.metadata (738 bytes)\n",
            "Downloading jdatetime-5.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading jalali_core-1.0.0-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: jalali-core, jdatetime\n",
            "Successfully installed jalali-core-1.0.0 jdatetime-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jdatetime\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "\n",
        "DATA_FILE_PATH = \"آمار نهایی روزانه استانی-11-02-1403 (csv).csv\"\n",
        "STATION_ID_COL = 'ایستگاه'\n",
        "TARGET_COL = 'ماكزيمم دما'\n",
        "\n",
        "S_YEAR_COL = 'سال شمسی'\n",
        "S_MONTH_COL = 'ماه شمسی'\n",
        "S_DAY_COL = 'روز شمسی'\n",
        "S_JULIAN_DAY_COL = 'ژولیوسی شمسی'\n",
        "\n",
        "WIND_DIR_COL = 'سمت باد'\n",
        "\n",
        "COLS_TO_DROP = [\n",
        "    'تبخير',\n",
        "    'ميانگين فشار QFF',\n",
        "    'تاریخ میلادی',\n",
        "    'سال میلادی',\n",
        "    'ماه میلادی',\n",
        "    'روز میلادی',\n",
        "    'ژولیوسی میلادی',\n",
        "    'تاریخ شمسی',\n",
        "    'روزها'\n",
        "]\n",
        "\n",
        "LOOKBACK_WINDOW = 30\n",
        "FORECAST_HORIZON = 7\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALIDATION_RATIO = 0.15\n",
        "\n",
        "def is_valid_jalali_date(year, month, day):\n",
        "    if not (isinstance(year, (int, float)) and isinstance(month, (int, float)) and isinstance(day, (int, float))):\n",
        "        return False\n",
        "    if np.isnan(year) or np.isnan(month) or np.isnan(day):\n",
        "        return False\n",
        "\n",
        "    year, month, day = int(year), int(month), int(day)\n",
        "    if not (1 <= month <= 12 and 1 <= day <= 31): return False\n",
        "    if month <= 6 and day > 31: return False\n",
        "    if 7 <= month <= 11 and day > 30: return False\n",
        "    if month == 12:\n",
        "        try:\n",
        "            is_leap = jdatetime.date(year, 1, 1).isleap()\n",
        "        except ValueError:\n",
        "            return False\n",
        "        if (is_leap and day > 30) or (not is_leap and day > 29):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def preprocess_data(file_path,\n",
        "                    station_id_col,\n",
        "                    s_year_col, s_month_col, s_day_col, s_julian_day_col,\n",
        "                    wind_dir_col,\n",
        "                    target_col,\n",
        "                    cols_to_drop,\n",
        "                    train_ratio, val_ratio,\n",
        "                    lookback_window, forecast_horizon):\n",
        "    try:\n",
        "        na_values_list = ['***', '---', '#N/A', 'N/A', 'NULL', 'nan', 'NaN', 'None', '', ' ']\n",
        "        df = pd.read_csv(file_path, low_memory=False, na_values=na_values_list)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\"); return None\n",
        "    print(f\"Initial data loaded. Shape: {df.shape}\\nInitial columns: {df.columns.tolist()}\")\n",
        "\n",
        "    actual_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "    if actual_cols_to_drop:\n",
        "        df = df.drop(columns=actual_cols_to_drop)\n",
        "    print(f\"Columns after initial drop ({len(actual_cols_to_drop)} columns): {df.columns.tolist()}\")\n",
        "\n",
        "    critical_cols_map = {\n",
        "        station_id_col: str, s_year_col: float, s_month_col: float, s_day_col: float,\n",
        "        s_julian_day_col: float, target_col: float\n",
        "    }\n",
        "    if wind_dir_col in df.columns:\n",
        "        critical_cols_map[wind_dir_col] = float\n",
        "\n",
        "    other_numeric_covariates = [\n",
        "        'مينيمم دما', 'ميانگين دما', 'ماكزيمم رطوبت', 'مينيمم رطوبت',\n",
        "        'ميانگين رطوبت', 'بارندگي', 'ساعات آفتابي', 'حداكثر سرعت باد'\n",
        "    ]\n",
        "    for col in other_numeric_covariates:\n",
        "        if col in df.columns:\n",
        "            critical_cols_map[col] = float\n",
        "\n",
        "    for col, col_type in critical_cols_map.items():\n",
        "        if col in df.columns:\n",
        "            if col_type == str:\n",
        "                df[col] = df[col].astype(str)\n",
        "            else:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        elif col in [target_col, s_year_col, s_month_col, s_day_col, station_id_col]:\n",
        "            print(f\"Error: Critical and essential column '{col}' not found. Program will stop.\")\n",
        "            return None\n",
        "        else:\n",
        "            print(f\"Warning: Expected column '{col}' not found, but continuing.\")\n",
        "\n",
        "    essential_dropna_subset = [station_id_col, s_year_col, s_month_col, s_day_col, target_col]\n",
        "    if s_julian_day_col in df.columns: essential_dropna_subset.append(s_julian_day_col)\n",
        "    df.dropna(subset=essential_dropna_subset, inplace=True)\n",
        "\n",
        "    if wind_dir_col in df.columns and df[wind_dir_col].isnull().any():\n",
        "        print(f\"NaN values in column '{wind_dir_col}' were replaced with 0 (before sin/cos).\")\n",
        "        df[wind_dir_col] = df[wind_dir_col].fillna(0)\n",
        "\n",
        "    print(f\"Data shape after removing initial NaNs in critical columns: {df.shape}\")\n",
        "    if df.empty: print(\"Error: No data remaining after removing initial NaNs.\"); return None\n",
        "\n",
        "    def get_jalali_weekday(row):\n",
        "        if pd.isna(row[s_year_col]) or pd.isna(row[s_month_col]) or pd.isna(row[s_day_col]):\n",
        "            return np.nan\n",
        "        if is_valid_jalali_date(row[s_year_col], row[s_month_col], row[s_day_col]):\n",
        "            return jdatetime.date(int(row[s_year_col]), int(row[s_month_col]), int(row[s_day_col])).weekday()\n",
        "        return np.nan\n",
        "    df['derived_day_of_week'] = df.apply(get_jalali_weekday, axis=1)\n",
        "    df.dropna(subset=['derived_day_of_week'], inplace=True)\n",
        "    df['derived_day_of_week'] = df['derived_day_of_week'].astype(int)\n",
        "    print(f\"Data shape after creating and cleaning derived_day_of_week: {df.shape}\")\n",
        "    if df.empty: print(\"Error: No data remaining after processing derived_day_of_week.\"); return None\n",
        "\n",
        "    if s_julian_day_col in df.columns:\n",
        "        df['day_of_year_sin'] = np.sin(2 * np.pi * df[s_julian_day_col] / 365.25)\n",
        "        df['day_of_year_cos'] = np.cos(2 * np.pi * df[s_julian_day_col] / 365.25)\n",
        "    else:\n",
        "        print(f\"Warning: Column '{s_julian_day_col}' (day of year) not found.\")\n",
        "\n",
        "    if df[station_id_col].isnull().any():\n",
        "        df[station_id_col] = df[station_id_col].fillna('UNKNOWN_STATION')\n",
        "    station_encoder = LabelEncoder()\n",
        "    encoded_station_id_col_name = f'{station_id_col}_encoded'\n",
        "    df[encoded_station_id_col_name] = station_encoder.fit_transform(df[station_id_col])\n",
        "    num_unique_stations = df[encoded_station_id_col_name].nunique()\n",
        "    print(f\"Number of unique stations encoded: {num_unique_stations}\")\n",
        "\n",
        "    if wind_dir_col in df.columns:\n",
        "        df[f'{wind_dir_col}_sin'] = np.sin(2 * np.pi * df[wind_dir_col] / 360.0)\n",
        "        df[f'{wind_dir_col}_cos'] = np.cos(2 * np.pi * df[wind_dir_col] / 360.0)\n",
        "        if wind_dir_col in df.columns: df = df.drop(columns=[wind_dir_col])\n",
        "        print(f\"Column '{wind_dir_col}' was cyclically transformed.\")\n",
        "    else:\n",
        "        print(f\"Warning: Column '{wind_dir_col}' not found.\")\n",
        "\n",
        "    df['temp_jalali_int_date'] = df.apply(\n",
        "        lambda row: int(f\"{int(row[s_year_col]):04d}{int(row[s_month_col]):02d}{int(row[s_day_col]):02d}\")\n",
        "        if is_valid_jalali_date(row[s_year_col], row[s_month_col], row[s_day_col]) else np.nan, axis=1)\n",
        "    df.dropna(subset=['temp_jalali_int_date'], inplace=True)\n",
        "    if not df.empty:\n",
        "        df['temp_jalali_int_date'] = df['temp_jalali_int_date'].astype(int)\n",
        "        df = df.sort_values(by=[encoded_station_id_col_name, 'temp_jalali_int_date']).drop(columns=['temp_jalali_int_date'])\n",
        "        df = df.reset_index(drop=True)\n",
        "    print(f\"Data shape after final sorting and date cleaning: {df.shape}\")\n",
        "    if df.empty: print(\"Error: No data remaining after date sorting.\"); return None\n",
        "\n",
        "    cols_not_to_impute = [encoded_station_id_col_name, s_year_col, s_month_col, s_day_col, 'derived_day_of_week']\n",
        "    if s_julian_day_col in df.columns: cols_not_to_impute.append(s_julian_day_col)\n",
        "\n",
        "    cols_for_imputation = df.select_dtypes(include=np.number).columns.difference(cols_not_to_impute).tolist()\n",
        "\n",
        "    for col in cols_for_imputation:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            df[col] = df.groupby(encoded_station_id_col_name, group_keys=False)[col].apply(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
        "            df[col] = df.groupby(encoded_station_id_col_name, group_keys=False)[col].ffill().bfill()\n",
        "\n",
        "            current_col_median = df[col].median()\n",
        "            if pd.isna(current_col_median):\n",
        "                print(f\"Warning: Median for column '{col}' (from overall df) could not be calculated. Filling with 0.\")\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(current_col_median)\n",
        "\n",
        "    for col in [s_month_col, s_day_col, 'derived_day_of_week']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    nan_counts = df.isnull().sum()\n",
        "    print(f\"NaN counts after all filling stages:\\n{nan_counts[nan_counts > 0]}\")\n",
        "\n",
        "    train_dfs, val_dfs, test_dfs = [], [], []\n",
        "    grouped = df.groupby(encoded_station_id_col_name)\n",
        "    for station_code_encoded, group_df in grouped:\n",
        "        n = len(group_df)\n",
        "        min_len_for_a_sequence = lookback_window + forecast_horizon\n",
        "        if n < min_len_for_a_sequence + 1:\n",
        "            print(f\"Station {station_encoder.inverse_transform([station_code_encoded])[0]} has very little data ({n} rows) and will be ignored.\")\n",
        "            continue\n",
        "        train_end_idx = int(n * train_ratio)\n",
        "        val_end_idx = train_end_idx + int(n * val_ratio)\n",
        "\n",
        "        if not (train_end_idx >= min_len_for_a_sequence and \\\n",
        "            (val_end_idx - train_end_idx) >= min_len_for_a_sequence and \\\n",
        "            (n - val_end_idx) >= min_len_for_a_sequence):\n",
        "            if n >= min_len_for_a_sequence :\n",
        "                print(f\"Station {station_encoder.inverse_transform([station_code_encoded])[0]} ({n} rows) was added only to training.\")\n",
        "                train_dfs.append(group_df)\n",
        "            else:\n",
        "                print(f\"Station {station_encoder.inverse_transform([station_code_encoded])[0]} does not have enough data to create a sequence.\")\n",
        "            continue\n",
        "        train_dfs.append(group_df.iloc[:train_end_idx])\n",
        "        val_dfs.append(group_df.iloc[train_end_idx:val_end_idx])\n",
        "        test_dfs.append(group_df.iloc[val_end_idx:])\n",
        "\n",
        "    if not train_dfs:\n",
        "        print(\"Error: No data left for training after splitting.\")\n",
        "        return None\n",
        "\n",
        "    df_train = pd.concat(train_dfs).reset_index(drop=True)\n",
        "    df_val = pd.concat(val_dfs).reset_index(drop=True) if val_dfs else pd.DataFrame(columns=df_train.columns)\n",
        "    df_test = pd.concat(test_dfs).reset_index(drop=True) if test_dfs else pd.DataFrame(columns=df_train.columns)\n",
        "\n",
        "    print(f\"Training set size: {df_train.shape}, Validation: {df_val.shape}, Test: {df_test.shape}\")\n",
        "    if df_train.empty:\n",
        "        print(\"Error: Training set is empty.\"); return None\n",
        "\n",
        "    past_cont_cov_cols_actual = ['مينيمم دما', 'ميانگين دما', 'ماكزيمم رطوبت', 'مينيمم رطوبت',\n",
        "                                'بارندگي', 'ساعات آفتابي', 'حداكثر سرعت باد']\n",
        "    if f'{wind_dir_col}_sin' in df.columns:\n",
        "        past_cont_cov_cols_actual.extend([f'{wind_dir_col}_sin', f'{wind_dir_col}_cos'])\n",
        "\n",
        "    final_past_cont_cov_cols = [col for col in past_cont_cov_cols_actual if col in df_train.columns]\n",
        "    print(f\"Final list of past continuous covariates for scaling and sequence: {final_past_cont_cov_cols}, Count: {len(final_past_cont_cov_cols)}\")\n",
        "\n",
        "    future_cont_cov_cols_actual = []\n",
        "    if 'day_of_year_sin' in df.columns: future_cont_cov_cols_actual.append('day_of_year_sin')\n",
        "    if 'day_of_year_cos' in df.columns: future_cont_cov_cols_actual.append('day_of_year_cos')\n",
        "    if s_year_col in df.columns: future_cont_cov_cols_actual.append(s_year_col)\n",
        "    final_future_cont_cov_cols = [col for col in future_cont_cov_cols_actual if col in df_train.columns]\n",
        "\n",
        "    final_future_cat_cov_cols = [col for col in [s_month_col, s_day_col, 'derived_day_of_week'] if col in df_train.columns]\n",
        "\n",
        "    cols_to_scale = [target_col] + final_past_cont_cov_cols + final_future_cont_cov_cols\n",
        "    cols_to_scale = sorted(list(set(col for col in cols_to_scale if col in df_train.columns)))\n",
        "\n",
        "    scalers = {}\n",
        "    print(f\"Columns selected for scaling: {cols_to_scale}\")\n",
        "    for col in cols_to_scale:\n",
        "        scaler = StandardScaler()\n",
        "        col_median_train = df_train[col].median()\n",
        "        if pd.isna(col_median_train): col_median_train = 0\n",
        "\n",
        "        current_train_col_data = pd.to_numeric(df_train[col], errors='coerce').fillna(col_median_train)\n",
        "        df_train[col] = scaler.fit_transform(current_train_col_data.values.reshape(-1,1))\n",
        "\n",
        "        if col in df_val.columns and not df_val.empty:\n",
        "            current_val_col_data = pd.to_numeric(df_val[col], errors='coerce').fillna(col_median_train)\n",
        "            df_val[col] = scaler.transform(current_val_col_data.values.reshape(-1,1))\n",
        "        if col in df_test.columns and not df_test.empty:\n",
        "            current_test_col_data = pd.to_numeric(df_test[col], errors='coerce').fillna(col_median_train)\n",
        "            df_test[col] = scaler.transform(current_test_col_data.values.reshape(-1,1))\n",
        "        scalers[col] = scaler\n",
        "\n",
        "    for df_set_name, df_set in zip([\"Training\", \"Validation\", \"Test\"], [df_train, df_val, df_test]):\n",
        "        if not df_set.empty and target_col not in df_set.columns:\n",
        "            print(f\"Critical Error: Target column '{target_col}' not found in {df_set_name} set. Available columns: {df_set.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "    def create_sequences_fn(data, station_id_col_encoded_arg, target_col_arg,\n",
        "                            future_cat_cols_arg, future_cont_cols_arg,\n",
        "                            past_cont_cov_cols_arg,\n",
        "                            lookback, horizon):\n",
        "        sequences = {\n",
        "            'x_static_cat': [], 'x_past_target': [], 'x_past_cov_cont': [],\n",
        "            'x_future_known_cat': [], 'x_future_known_cont': [], 'y_target': []\n",
        "        }\n",
        "        num_samples_created = 0\n",
        "        if data.empty: pass\n",
        "        else:\n",
        "            grouped_data = data.groupby(station_id_col_encoded_arg)\n",
        "            for _, group in grouped_data:\n",
        "                group = group.reset_index(drop=True)\n",
        "                len_ts = len(group)\n",
        "                if len_ts < lookback + horizon: continue\n",
        "\n",
        "                for i in range(len_ts - lookback - horizon + 1):\n",
        "                    num_samples_created +=1\n",
        "                    past_end_idx = i + lookback\n",
        "                    future_end_idx = past_end_idx + horizon\n",
        "\n",
        "                    sequences['x_static_cat'].append(group.loc[i, station_id_col_encoded_arg])\n",
        "                    sequences['x_past_target'].append(group.loc[i:past_end_idx-1, target_col_arg].values.astype(float).reshape(lookback, 1))\n",
        "\n",
        "                    if past_cont_cov_cols_arg:\n",
        "                        sequences['x_past_cov_cont'].append(group.loc[i:past_end_idx-1, past_cont_cov_cols_arg].values.astype(float))\n",
        "                    else:\n",
        "                        sequences['x_past_cov_cont'].append(np.empty((lookback, 0), dtype=float))\n",
        "\n",
        "                    current_future_cat_vals = []\n",
        "                    if future_cat_cols_arg:\n",
        "                        for col in future_cat_cols_arg:\n",
        "                            if col in group.columns:\n",
        "                                current_future_cat_vals.append(group.loc[past_end_idx:future_end_idx-1, col].values.astype(int).reshape(horizon,1))\n",
        "                            else:\n",
        "                                print(f\"Warning in create_sequences: Future categorical column '{col}' not found in group.\")\n",
        "                                current_future_cat_vals.append(np.full((horizon,1), 0, dtype=int))\n",
        "                    if current_future_cat_vals:\n",
        "                        sequences['x_future_known_cat'].append(np.concatenate(current_future_cat_vals, axis=1))\n",
        "                    else:\n",
        "                        sequences['x_future_known_cat'].append(np.empty((horizon, 0), dtype=int))\n",
        "\n",
        "                    current_future_cont_vals = []\n",
        "                    if future_cont_cols_arg:\n",
        "                        for col in future_cont_cols_arg:\n",
        "                            if col in group.columns:\n",
        "                                current_future_cont_vals.append(group.loc[past_end_idx:future_end_idx-1, col].values.astype(float).reshape(horizon,1))\n",
        "                            else:\n",
        "                                print(f\"Warning in create_sequences: Future continuous column '{col}' not found in group.\")\n",
        "                                current_future_cont_vals.append(np.zeros((horizon,1), dtype=float))\n",
        "                    if current_future_cont_vals:\n",
        "                        sequences['x_future_known_cont'].append(np.concatenate(current_future_cont_vals, axis=1))\n",
        "                    else:\n",
        "                        sequences['x_future_known_cont'].append(np.empty((horizon, 0), dtype=float))\n",
        "\n",
        "                    sequences['y_target'].append(group.loc[past_end_idx:future_end_idx-1, target_col_arg].values.astype(float).reshape(horizon, 1))\n",
        "\n",
        "        for key in sequences:\n",
        "            dtype_expected = int if key == 'x_static_cat' or key == 'x_future_known_cat' else float\n",
        "            if sequences[key]:\n",
        "                try:\n",
        "                    sequences[key] = np.array(sequences[key], dtype=dtype_expected)\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error converting '{key}' to NumPy with dtype {dtype_expected}: {e}. Attempting to fix...\")\n",
        "                    n_s = num_samples_created\n",
        "                    lkbk, hrzn = lookback, horizon\n",
        "                    n_pcc = len(past_cont_cov_cols_arg if past_cont_cov_cols_arg else [])\n",
        "                    n_fkc = len(future_cat_cols_arg if future_cat_cols_arg else [])\n",
        "                    n_fkcn = len(future_cont_cols_arg if future_cont_cols_arg else [])\n",
        "\n",
        "                    fallback_shapes = {\n",
        "                        'x_static_cat': (n_s,1), 'x_past_target': (n_s, lkbk, 1),\n",
        "                        'x_past_cov_cont': (n_s, lkbk, n_pcc),\n",
        "                        'x_future_known_cat': (n_s, hrzn, n_fkc),\n",
        "                        'x_future_known_cont': (n_s, hrzn, n_fkcn),\n",
        "                        'y_target': (n_s, hrzn, 1)\n",
        "                    }\n",
        "                    sequences[key] = np.empty(fallback_shapes.get(key,(0,)), dtype=dtype_expected)\n",
        "\n",
        "            else:\n",
        "                n_s = num_samples_created\n",
        "                lkbk, hrzn = lookback, horizon\n",
        "                n_pcc = len(past_cont_cov_cols_arg if past_cont_cov_cols_arg else [])\n",
        "                n_fkc = len(future_cat_cols_arg if future_cat_cols_arg else [])\n",
        "                n_fkcn = len(future_cont_cols_arg if future_cont_cols_arg else [])\n",
        "\n",
        "                if key == 'x_static_cat': shape = (n_s,1)\n",
        "                elif key == 'x_past_target': shape = (n_s, lkbk, 1)\n",
        "                elif key == 'x_past_cov_cont': shape = (n_s, lkbk, n_pcc)\n",
        "                elif key == 'x_future_known_cat': shape = (n_s, hrzn, n_fkc)\n",
        "                elif key == 'x_future_known_cont': shape = (n_s, hrzn, n_fkcn)\n",
        "                elif key == 'y_target': shape = (n_s, hrzn, 1)\n",
        "                else: shape = (0,)\n",
        "                sequences[key] = np.empty(shape, dtype=dtype_expected)\n",
        "\n",
        "            if key == 'x_static_cat' and sequences[key].ndim == 1 and sequences[key].size > 0:\n",
        "                sequences[key] = sequences[key].reshape(-1, 1)\n",
        "        return sequences\n",
        "\n",
        "    print(\"\\nCreating sequences for training set...\")\n",
        "    train_sequences = create_sequences_fn(df_train, encoded_station_id_col_name, target_col,\n",
        "                                        final_future_cat_cov_cols, final_future_cont_cov_cols,\n",
        "                                        final_past_cont_cov_cols,\n",
        "                                        lookback_window, forecast_horizon)\n",
        "    print(\"Creating sequences for validation set...\")\n",
        "    val_sequences = create_sequences_fn(df_val, encoded_station_id_col_name, target_col,\n",
        "                                        final_future_cat_cov_cols, final_future_cont_cov_cols,\n",
        "                                        final_past_cont_cov_cols,\n",
        "                                        lookback_window, forecast_horizon)\n",
        "    print(\"Creating sequences for test set...\")\n",
        "    test_sequences = create_sequences_fn(df_test, encoded_station_id_col_name, target_col,\n",
        "                                        final_future_cat_cov_cols, final_future_cont_cov_cols,\n",
        "                                        final_past_cont_cov_cols,\n",
        "                                        lookback_window, forecast_horizon)\n",
        "\n",
        "    final_params = {\n",
        "        'target_dim': 1,\n",
        "        'static_categorical_input_dims': [num_unique_stations] if num_unique_stations > 0 and not df_train.empty and (encoded_station_id_col_name in df_train.columns and df_train[encoded_station_id_col_name].nunique() > 0) else [],\n",
        "        'static_continuous_input_dim': 0,\n",
        "        'obs_continuous_input_dim': len(final_past_cont_cov_cols),\n",
        "        'obs_categorical_input_dims': [],\n",
        "        'known_categorical_input_dims': [\n",
        "            max(1, df_train[col].nunique(dropna=False)) if col in df_train.columns and not df_train.empty and df_train[col].nunique(dropna=False) > 0 else (12 if col == s_month_col else (31 if col == s_day_col else 7))\n",
        "            for col in final_future_cat_cov_cols\n",
        "        ] if final_future_cat_cov_cols else [],\n",
        "        'known_continuous_input_dim': len(final_future_cont_cov_cols)\n",
        "    }\n",
        "    if not final_params['static_categorical_input_dims'] and num_unique_stations > 0 :\n",
        "            final_params['static_categorical_input_dims'] = []\n",
        "    if final_params['known_categorical_input_dims'] and any(c == 0 for c in final_params['known_categorical_input_dims']):\n",
        "        final_params['known_categorical_input_dims'] = [max(1,c) if c is not None else 1 for c in final_params['known_categorical_input_dims']]\n",
        "\n",
        "    print(\"\\nCalculated parameters for TFT model:\")\n",
        "    for key, value in final_params.items(): print(f\"  {key}: {value}\")\n",
        "\n",
        "    return train_sequences, val_sequences, test_sequences, scalers, station_encoder, final_params\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processed_output = preprocess_data(\n",
        "        file_path=DATA_FILE_PATH,\n",
        "        station_id_col=STATION_ID_COL,\n",
        "        s_year_col=S_YEAR_COL, s_month_col=S_MONTH_COL, s_day_col=S_DAY_COL,\n",
        "        s_julian_day_col=S_JULIAN_DAY_COL,\n",
        "        wind_dir_col=WIND_DIR_COL, target_col=TARGET_COL,\n",
        "        cols_to_drop=COLS_TO_DROP,\n",
        "        train_ratio=TRAIN_RATIO, val_ratio=VALIDATION_RATIO,\n",
        "        lookback_window=LOOKBACK_WINDOW, forecast_horizon=FORECAST_HORIZON\n",
        "    )\n",
        "\n",
        "    if processed_output:\n",
        "        train_data, val_data, test_data, data_scalers, st_encoder, model_params_from_preprocessing = processed_output\n",
        "        print(\"\\nSample shape of training data (if created):\")\n",
        "        for key, arr in train_data.items():\n",
        "            if isinstance(arr, np.ndarray):\n",
        "                print(f\"  {key}: {arr.shape}\")\n",
        "            else:\n",
        "                print(f\"  {key}: is not a numpy array (type: {type(arr)})\")\n",
        "\n",
        "EPSILON = 1e-8\n",
        "\n",
        "class GatedLinearUnit(nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, output_size)\n",
        "        self.linear2 = nn.Linear(input_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.sigmoid(self.linear1(x)) * self.linear2(x)\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int,\n",
        "                dropout_rate: float, context_size: Optional[int] = None,\n",
        "                is_output_layer: bool = False):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size; self.hidden_size = hidden_size; self.output_size = output_size\n",
        "        self.context_size = context_size; self.dropout_rate = dropout_rate; self.is_output_layer = is_output_layer\n",
        "        self.W2_a = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.W3_c = nn.Linear(self.context_size, self.hidden_size, bias=False) if self.context_size is not None else None\n",
        "        self.elu = nn.ELU(); self.dropout = nn.Dropout(self.dropout_rate)\n",
        "        self.W1_eta2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.glu_or_linear = nn.Linear(self.hidden_size, self.output_size) if self.is_output_layer else GatedLinearUnit(self.hidden_size, self.output_size)\n",
        "        self.skip_connection_projector = nn.Linear(self.input_size, self.output_size) if self.input_size != self.output_size else None\n",
        "        self.layer_norm = nn.LayerNorm(self.output_size)\n",
        "    def forward(self, a: torch.Tensor, c: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        eta_2_intermediate = self.W2_a(a)\n",
        "        if c is not None and self.W3_c is not None: eta_2_intermediate += self.W3_c(c)\n",
        "        eta_2 = self.elu(eta_2_intermediate)\n",
        "        eta_2_dropout = self.dropout(eta_2); eta_1 = self.W1_eta2(eta_2_dropout)\n",
        "        gated_output = self.glu_or_linear(eta_1); gated_output_dropout = self.dropout(gated_output)\n",
        "        skip_a = self.skip_connection_projector(a) if self.skip_connection_projector is not None else a\n",
        "        return self.layer_norm(skip_a + gated_output_dropout)\n",
        "\n",
        "class VariableSelectionNetwork(nn.Module):\n",
        "    def __init__(self, num_total_inputs: int, hidden_size: int, dropout_rate: float,\n",
        "                context_size: Optional[int] = None, is_static: bool = False):\n",
        "        super().__init__()\n",
        "        if num_total_inputs <= 0: raise ValueError(\"num_total_inputs for VSN must be positive.\")\n",
        "        self.num_total_inputs = num_total_inputs; self.hidden_size = hidden_size; self.dropout_rate = dropout_rate\n",
        "        self.context_size = context_size; self.is_static = is_static\n",
        "        self.per_variable_grns = nn.ModuleList([\n",
        "            GatedResidualNetwork(hidden_size, hidden_size, hidden_size, dropout_rate) for _ in range(num_total_inputs)])\n",
        "        flat_input_dim_for_weights_grn = num_total_inputs * hidden_size\n",
        "        grn_context_size_for_weights = self.context_size if not self.is_static and self.context_size is not None else None\n",
        "        self.weight_calculator_grn = GatedResidualNetwork(\n",
        "            flat_input_dim_for_weights_grn, hidden_size, num_total_inputs, dropout_rate,\n",
        "            context_size=grn_context_size_for_weights, is_output_layer=False)\n",
        "        self.softmax = nn.Softmax(dim=-1); self.attention_weights: Optional[torch.Tensor] = None\n",
        "    def forward(self, all_variables_projected: torch.Tensor,\n",
        "                context_for_weights: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        batch_size, seq_len, num_vars_check, var_hidden_size = all_variables_projected.shape\n",
        "        if num_vars_check != self.num_total_inputs:\n",
        "            raise ValueError(f\"VSN expected {self.num_total_inputs} vars, got {num_vars_check} (shape: {all_variables_projected.shape})\")\n",
        "        processed_variables_list = [\n",
        "            self.per_variable_grns[i](all_variables_projected[:, :, i, :].reshape(batch_size * seq_len, var_hidden_size)).reshape(batch_size, seq_len, 1, var_hidden_size)\n",
        "            for i in range(self.num_total_inputs)]\n",
        "        processed_variables_stacked = torch.cat(processed_variables_list, dim=2)\n",
        "        flat_original_vars_for_weights = all_variables_projected.reshape(batch_size * seq_len, self.num_total_inputs * var_hidden_size)\n",
        "        expanded_context = None\n",
        "        if context_for_weights is not None and not self.is_static:\n",
        "            expanded_context = context_for_weights.unsqueeze(1).expand(-1, seq_len, -1).reshape(batch_size * seq_len, -1)\n",
        "        variable_weights_grn_out = self.weight_calculator_grn(flat_original_vars_for_weights, expanded_context)\n",
        "        variable_weights = self.softmax(variable_weights_grn_out)\n",
        "        variable_weights_reshaped = variable_weights.reshape(batch_size, seq_len, self.num_total_inputs)\n",
        "        self.attention_weights = variable_weights_reshaped\n",
        "        weighted_sum_of_processed_vars = torch.sum(\n",
        "            processed_variables_stacked * variable_weights_reshaped.unsqueeze(-1), dim=2)\n",
        "        return weighted_sum_of_processed_vars, self.attention_weights\n",
        "\n",
        "class InterpretableMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        if d_model % num_heads != 0: raise ValueError(\"d_model must be divisible by num_heads\")\n",
        "        self.d_model = d_model; self.num_heads = num_heads; self.d_k = d_model // num_heads\n",
        "        self.d_v_shared = d_model // num_heads\n",
        "        self.W_q_list = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(num_heads)])\n",
        "        self.W_k_list = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(num_heads)])\n",
        "        self.W_v_shared = nn.Linear(d_model, self.d_v_shared, bias=False)\n",
        "        self.W_out = nn.Linear(self.d_v_shared, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate); self.scale_factor = self.d_k**-0.5\n",
        "        self.attention_scores: Optional[torch.Tensor] = None\n",
        "    def forward(self, q_input: torch.Tensor, k_input: torch.Tensor, v_input: torch.Tensor,\n",
        "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        shared_v_projected = self.W_v_shared(v_input)\n",
        "        head_outputs_list = []; head_attentions_list = []\n",
        "        for i in range(self.num_heads):\n",
        "            Q_i = self.W_q_list[i](q_input); K_i = self.W_k_list[i](k_input)\n",
        "            attn_scores_i = torch.matmul(Q_i, K_i.transpose(-2, -1)) * self.scale_factor\n",
        "            if mask is not None:\n",
        "                final_mask_for_head = mask\n",
        "                if mask.ndim == 4:\n",
        "                    final_mask_for_head = mask[:, i, :, :]\n",
        "                attn_scores_i = attn_scores_i.masked_fill(final_mask_for_head == 0, -float('inf'))\n",
        "\n",
        "            attn_probs_i = F.softmax(attn_scores_i, dim=-1); attn_probs_i = self.dropout(attn_probs_i)\n",
        "            head_attentions_list.append(attn_probs_i.unsqueeze(1))\n",
        "            output_i = torch.matmul(attn_probs_i, shared_v_projected)\n",
        "            head_outputs_list.append(output_i)\n",
        "        self.attention_scores = torch.cat(head_attentions_list, dim=1)\n",
        "        summed_heads_output = torch.stack(head_outputs_list, dim=0).sum(dim=0)\n",
        "        return self.W_out(summed_heads_output)\n",
        "\n",
        "class TemporalFusionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                target_dim: int, static_categorical_input_dims: List[int], static_continuous_input_dim: int,\n",
        "                obs_categorical_input_dims: List[int], obs_continuous_input_dim: int,\n",
        "                known_categorical_input_dims: List[int], known_continuous_input_dim: int,\n",
        "                d_model: int, num_attention_heads: int, lstm_hidden_layers: int,\n",
        "                dropout_rate: float, lookback_window_size: int, forecast_horizon_size: int,\n",
        "                output_quantiles: List[float]):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model; self.num_attention_heads = num_attention_heads\n",
        "        self.lstm_hidden_layers = lstm_hidden_layers; self.dropout_rate = dropout_rate\n",
        "        self.lookback_window_size = lookback_window_size; self.forecast_horizon_size = forecast_horizon_size\n",
        "        self.output_quantiles = output_quantiles; self.num_quantiles = len(output_quantiles)\n",
        "\n",
        "        self.static_cat_embed_layers = nn.ModuleList([nn.Embedding(card, d_model) for card in static_categorical_input_dims if card > 0])\n",
        "        self.static_cont_linear_layer = nn.Linear(static_continuous_input_dim, d_model) if static_continuous_input_dim > 0 else None\n",
        "        self.past_target_projection = nn.Linear(target_dim, d_model)\n",
        "        self.past_cat_cov_embed_layers = nn.ModuleList([nn.Embedding(card, d_model) for card in obs_categorical_input_dims if card > 0])\n",
        "        self.past_cont_cov_projection_layers = nn.ModuleList([nn.Linear(1, d_model) for _ in range(obs_continuous_input_dim)])\n",
        "        self.future_cat_cov_embed_layers = nn.ModuleList([nn.Embedding(card, d_model) for card in known_categorical_input_dims if card > 0])\n",
        "        self.future_cont_cov_projection_layers = nn.ModuleList([nn.Linear(1, d_model) for _ in range(known_continuous_input_dim)])\n",
        "\n",
        "        self.num_static_vars_for_vsn = len(self.static_cat_embed_layers) + (1 if self.static_cont_linear_layer is not None else 0)\n",
        "        self.num_past_vars_for_vsn = 1 + len(self.past_cat_cov_embed_layers) + len(self.past_cont_cov_projection_layers)\n",
        "        self.num_future_vars_for_vsn = len(self.future_cat_cov_embed_layers) + len(self.future_cont_cov_projection_layers)\n",
        "\n",
        "        self.static_vsn = VariableSelectionNetwork(self.num_static_vars_for_vsn, d_model, dropout_rate, is_static=True) if self.num_static_vars_for_vsn > 0 else None\n",
        "        if self.num_past_vars_for_vsn <=0: raise ValueError(\"Past VSN must have inputs.\")\n",
        "        self.past_inputs_vsn = VariableSelectionNetwork(self.num_past_vars_for_vsn, d_model, dropout_rate, context_size=d_model)\n",
        "        self.future_inputs_vsn = VariableSelectionNetwork(self.num_future_vars_for_vsn, d_model, dropout_rate, context_size=d_model) if self.num_future_vars_for_vsn > 0 else None\n",
        "\n",
        "        self.grn_c_s = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate); self.grn_c_c = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate)\n",
        "        self.grn_c_h = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate); self.grn_c_e = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate)\n",
        "        self.past_lstm = nn.LSTM(d_model, d_model, lstm_hidden_layers, batch_first=True, dropout=dropout_rate if lstm_hidden_layers > 1 else 0)\n",
        "        self.future_lstm = nn.LSTM(d_model, d_model, lstm_hidden_layers, batch_first=True, dropout=dropout_rate if lstm_hidden_layers > 1 else 0) if self.num_future_vars_for_vsn > 0 else None\n",
        "        self.locality_enhancement_glu = GatedLinearUnit(d_model,d_model); self.locality_enhancement_norm = nn.LayerNorm(d_model)\n",
        "        self.static_enrichment_grn = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate,context_size=d_model)\n",
        "        self.multihead_attention = InterpretableMultiHeadAttention(d_model,num_attention_heads,dropout_rate)\n",
        "        self.attention_gated_skip = GatedLinearUnit(d_model,d_model); self.attention_norm = nn.LayerNorm(d_model)\n",
        "        self.position_wise_ff_grn = GatedResidualNetwork(d_model,d_model,d_model,dropout_rate)\n",
        "        self.decoder_block_glu = GatedLinearUnit(d_model,d_model); self.decoder_block_norm = nn.LayerNorm(d_model)\n",
        "        self.output_projection = nn.Linear(d_model, self.num_quantiles)\n",
        "        self.static_vsn_weights: Optional[torch.Tensor]=None; self.past_vsn_weights: Optional[torch.Tensor]=None\n",
        "        self.future_vsn_weights: Optional[torch.Tensor]=None; self.attention_matrices: Optional[torch.Tensor]=None\n",
        "\n",
        "    def _project_and_stack_inputs_for_vsn(self,\n",
        "                                        target_tensor: Optional[torch.Tensor], cat_cov_tensor: Optional[torch.Tensor],\n",
        "                                        cont_cov_tensor: Optional[torch.Tensor], target_proj_layer: Optional[nn.Linear],\n",
        "                                        cat_embed_layers_list: nn.ModuleList, cont_proj_layers_list: nn.ModuleList,\n",
        "                                        batch_size: int, seq_len: int,\n",
        "                                        is_static: bool = False\n",
        "                                        ) -> Optional[torch.Tensor]:\n",
        "        projected_vars_list = []\n",
        "        if target_tensor is not None and target_proj_layer is not None:\n",
        "            projected_vars_list.append(target_proj_layer(target_tensor).unsqueeze(2))\n",
        "        if cat_cov_tensor is not None and cat_embed_layers_list:\n",
        "            for i, embed_layer in enumerate(cat_embed_layers_list):\n",
        "                current_cat_input = cat_cov_tensor[:, i] if is_static else cat_cov_tensor[:, :, i]\n",
        "                embedded_var = embed_layer(current_cat_input)\n",
        "                if is_static: embedded_var = embedded_var.unsqueeze(1)\n",
        "                projected_vars_list.append(embedded_var.unsqueeze(2))\n",
        "        if cont_cov_tensor is not None and cont_proj_layers_list:\n",
        "            if is_static and len(cont_proj_layers_list) == 1 and self.static_cont_linear_layer is not None :\n",
        "                    projected_vars_list.append(self.static_cont_linear_layer(cont_cov_tensor).unsqueeze(1).unsqueeze(2))\n",
        "            else:\n",
        "                for i, proj_layer in enumerate(cont_proj_layers_list):\n",
        "                    current_cont_input = cont_cov_tensor[:, i:i+1] if is_static else cont_cov_tensor[:, :, i:i+1]\n",
        "                    projected_var = proj_layer(current_cont_input)\n",
        "                    if is_static: projected_var = projected_var.unsqueeze(1)\n",
        "                    projected_vars_list.append(projected_var.unsqueeze(2))\n",
        "        if not projected_vars_list: return None\n",
        "        return torch.cat(projected_vars_list, dim=2)\n",
        "\n",
        "    def forward(self,\n",
        "                s_cat: Optional[torch.Tensor], s_cont: Optional[torch.Tensor],\n",
        "                p_target: torch.Tensor, p_cat_cov: Optional[torch.Tensor], p_cont_cov: Optional[torch.Tensor],\n",
        "                f_cat_cov: Optional[torch.Tensor], f_cont_cov: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        batch_size, k_lookback, _ = p_target.shape; h_forecast = self.forecast_horizon_size; device = p_target.device\n",
        "        selected_static_context: torch.Tensor\n",
        "        if self.static_vsn is not None:\n",
        "            all_static_vars_projected = self._project_and_stack_inputs_for_vsn(\n",
        "                None, s_cat, s_cont, None, self.static_cat_embed_layers,\n",
        "                nn.ModuleList([self.static_cont_linear_layer]) if self.static_cont_linear_layer else nn.ModuleList(),\n",
        "                batch_size=batch_size, seq_len=1, is_static=True)\n",
        "            if all_static_vars_projected is None: raise ValueError(\"Static VSN error.\")\n",
        "            selected_static_features_context, static_vsn_w = self.static_vsn(all_static_vars_projected)\n",
        "            self.static_vsn_weights = static_vsn_w.squeeze(1); selected_static_context = selected_static_features_context.squeeze(1)\n",
        "        else: selected_static_context = torch.zeros((batch_size, self.d_model), device=device)\n",
        "        c_s=self.grn_c_s(selected_static_context); c_c=self.grn_c_c(selected_static_context); c_h=self.grn_c_h(selected_static_context); c_e=self.grn_c_e(selected_static_context)\n",
        "        all_past_vars_projected = self._project_and_stack_inputs_for_vsn(\n",
        "            p_target, p_cat_cov, p_cont_cov, self.past_target_projection,\n",
        "            self.past_cat_cov_embed_layers, self.past_cont_cov_projection_layers,\n",
        "            batch_size=batch_size, seq_len=k_lookback, is_static=False)\n",
        "        if all_past_vars_projected is None: raise ValueError(\"Past VSN error.\")\n",
        "        selected_past_features, past_vsn_w = self.past_inputs_vsn(all_past_vars_projected, c_s)\n",
        "        self.past_vsn_weights = past_vsn_w\n",
        "        selected_future_features: torch.Tensor\n",
        "        if self.future_inputs_vsn is not None:\n",
        "            all_future_vars_projected = self._project_and_stack_inputs_for_vsn(\n",
        "                None, f_cat_cov, f_cont_cov, None, self.future_cat_cov_embed_layers,\n",
        "                self.future_cont_cov_projection_layers, batch_size=batch_size, seq_len=h_forecast, is_static=False)\n",
        "            if all_future_vars_projected is None: raise ValueError(\"Future VSN error.\")\n",
        "            selected_future_features, future_vsn_w = self.future_inputs_vsn(all_future_vars_projected, c_s)\n",
        "            self.future_vsn_weights = future_vsn_w\n",
        "        else: selected_future_features = torch.zeros((batch_size, h_forecast, self.d_model), device=device)\n",
        "        h_0_past = c_h.unsqueeze(0).repeat(self.lstm_hidden_layers,1,1); c_0_past = c_c.unsqueeze(0).repeat(self.lstm_hidden_layers,1,1)\n",
        "        past_lstm_out, (h_n_past, c_n_past) = self.past_lstm(selected_past_features, (h_0_past, c_0_past))\n",
        "        future_lstm_out: torch.Tensor\n",
        "        if self.future_lstm is not None: future_lstm_out, _ = self.future_lstm(selected_future_features, (h_n_past, c_n_past))\n",
        "        else: future_lstm_out = torch.zeros_like(selected_future_features)\n",
        "        phi_past = self.locality_enhancement_norm(selected_past_features + self.locality_enhancement_glu(past_lstm_out))\n",
        "        phi_future = self.locality_enhancement_norm(selected_future_features + self.locality_enhancement_glu(future_lstm_out))\n",
        "        phi_combined = torch.cat([phi_past, phi_future], dim=1)\n",
        "        expanded_c_e = c_e.unsqueeze(1).expand(-1, k_lookback + h_forecast, -1)\n",
        "        theta = self.static_enrichment_grn(phi_combined, expanded_c_e)\n",
        "        total_seq_len = k_lookback + h_forecast\n",
        "        attention_mask_tril = torch.tril(torch.ones(total_seq_len, total_seq_len, device=device, dtype=torch.bool))\n",
        "        decoder_mask = attention_mask_tril.unsqueeze(0)\n",
        "        beta_intermediate = self.multihead_attention(theta, theta, theta, mask=decoder_mask)\n",
        "        self.attention_matrices = self.multihead_attention.attention_scores\n",
        "        beta = self.attention_norm(theta + self.attention_gated_skip(beta_intermediate))\n",
        "        psi_tilde_intermediate = self.position_wise_ff_grn(beta)\n",
        "        psi_tilde_final = self.decoder_block_norm(phi_combined + self.decoder_block_glu(psi_tilde_intermediate))\n",
        "        psi_tilde_final_future = psi_tilde_final[:, k_lookback:, :]\n",
        "        return self.output_projection(psi_tilde_final_future)\n",
        "\n",
        "def quantile_loss(predictions: torch.Tensor, targets: torch.Tensor, quantiles: List[float]) -> torch.Tensor:\n",
        "    if targets.ndim == 2: targets = targets.unsqueeze(-1)\n",
        "    targets_expanded = targets.expand_as(predictions)\n",
        "    errors = targets_expanded - predictions\n",
        "    loss_sum = torch.tensor(0.0, device=predictions.device)\n",
        "    for i, q_val in enumerate(quantiles):\n",
        "        q = torch.tensor(q_val, device=predictions.device)\n",
        "        loss_q_per_element = torch.max((q - 1) * errors[..., i], q * errors[..., i])\n",
        "        loss_sum += loss_q_per_element.mean()\n",
        "    return loss_sum / len(quantiles)\n",
        "\n",
        "def calculate_rmse(predictions: torch.Tensor, targets: torch.Tensor, scaler: Optional[StandardScaler] = None) -> float:\n",
        "    if predictions.shape != targets.shape:\n",
        "        if targets.ndim == predictions.ndim + 1 and targets.shape[-1] == 1:\n",
        "            targets = targets.squeeze(-1)\n",
        "        if predictions.shape != targets.shape:\n",
        "                raise ValueError(f\"Shape mismatch: preds {predictions.shape}, targets {targets.shape}\")\n",
        "\n",
        "    preds_np = predictions.detach().cpu().numpy()\n",
        "    targets_np = targets.detach().cpu().numpy()\n",
        "\n",
        "    if scaler:\n",
        "        preds_np = scaler.inverse_transform(preds_np.reshape(-1,1)).reshape(preds_np.shape)\n",
        "        targets_np = scaler.inverse_transform(targets_np.reshape(-1,1)).reshape(targets_np.shape)\n",
        "\n",
        "    mse = np.mean((preds_np - targets_np)**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return float(rmse)\n",
        "\n",
        "def create_dataloader_from_sequences(sequences_dict: Dict[str, np.ndarray],\n",
        "                                    batch_size: int,\n",
        "                                    expected_params: Dict,\n",
        "                                    shuffle: bool = False):\n",
        "    tensor_keys_in_order_of_fwd_args = [\n",
        "        ('x_static_cat', 'static_categorical_input_dims', torch.long),\n",
        "        ('s_cont_placeholder', 'static_continuous_input_dim', torch.float),\n",
        "        ('x_past_target', 'target_dim', torch.float),\n",
        "        ('x_past_cov_cat', 'obs_categorical_input_dims', torch.long),\n",
        "        ('x_past_cov_cont', 'obs_continuous_input_dim', torch.float),\n",
        "        ('x_future_known_cat', 'known_categorical_input_dims', torch.long),\n",
        "        ('x_future_known_cont', 'known_continuous_input_dim', torch.float),\n",
        "        ('y_target', 'target_dim', torch.float)\n",
        "    ]\n",
        "\n",
        "    num_samples = -1\n",
        "    for key, _, _ in tensor_keys_in_order_of_fwd_args:\n",
        "        if sequences_dict.get(key) is not None and sequences_dict[key].size > 0:\n",
        "            num_samples = sequences_dict[key].shape[0]\n",
        "            break\n",
        "    if num_samples == -1:\n",
        "        print(\"Warning: No data found to create DataLoader.\")\n",
        "        return None, []\n",
        "\n",
        "    tensors_for_model_forward = {}\n",
        "\n",
        "    s_cat_data = sequences_dict.get('x_static_cat')\n",
        "    if s_cat_data is not None and s_cat_data.size > 0:\n",
        "        tensors_for_model_forward['s_cat'] = torch.from_numpy(s_cat_data).long()\n",
        "    else:\n",
        "        tensors_for_model_forward['s_cat'] = None\n",
        "\n",
        "    tensors_for_model_forward['s_cont'] = None\n",
        "\n",
        "    p_target_data = sequences_dict.get('x_past_target')\n",
        "    if p_target_data is not None and p_target_data.size > 0:\n",
        "        tensors_for_model_forward['p_target'] = torch.from_numpy(p_target_data).float()\n",
        "    else: raise ValueError(\"x_past_target (p_target) must not be empty.\")\n",
        "\n",
        "    tensors_for_model_forward['p_cat_cov'] = None\n",
        "\n",
        "    p_cont_cov_data = sequences_dict.get('x_past_cov_cont')\n",
        "    if p_cont_cov_data is not None and p_cont_cov_data.size > 0 and p_cont_cov_data.shape[-1] > 0:\n",
        "        tensors_for_model_forward['p_cont_cov'] = torch.from_numpy(p_cont_cov_data).float()\n",
        "    else:\n",
        "        tensors_for_model_forward['p_cont_cov'] = None\n",
        "\n",
        "    f_cat_cov_data = sequences_dict.get('x_future_known_cat')\n",
        "    if f_cat_cov_data is not None and f_cat_cov_data.size > 0 and f_cat_cov_data.shape[-1] > 0:\n",
        "        tensors_for_model_forward['f_cat_cov'] = torch.from_numpy(f_cat_cov_data).long()\n",
        "    else:\n",
        "        tensors_for_model_forward['f_cat_cov'] = None\n",
        "\n",
        "    f_cont_cov_data = sequences_dict.get('x_future_known_cont')\n",
        "    if f_cont_cov_data is not None and f_cont_cov_data.size > 0 and f_cont_cov_data.shape[-1] > 0:\n",
        "        tensors_for_model_forward['f_cont_cov'] = torch.from_numpy(f_cont_cov_data).float()\n",
        "    else:\n",
        "        tensors_for_model_forward['f_cont_cov'] = None\n",
        "\n",
        "    y_target_data = sequences_dict.get('y_target')\n",
        "    if y_target_data is not None and y_target_data.size > 0:\n",
        "        y_target_tensor = torch.from_numpy(y_target_data).float()\n",
        "    else: raise ValueError(\"y_target must not be empty.\")\n",
        "\n",
        "    dataset_tensors = []\n",
        "\n",
        "    if tensors_for_model_forward['s_cat'] is not None: dataset_tensors.append(tensors_for_model_forward['s_cat'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, 0, dtype=torch.long))\n",
        "\n",
        "    if tensors_for_model_forward['s_cont'] is not None: dataset_tensors.append(tensors_for_model_forward['s_cont'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, 0, dtype=torch.float))\n",
        "\n",
        "    dataset_tensors.append(tensors_for_model_forward['p_target'])\n",
        "\n",
        "    if tensors_for_model_forward['p_cat_cov'] is not None: dataset_tensors.append(tensors_for_model_forward['p_cat_cov'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, _LW, 0, dtype=torch.long))\n",
        "\n",
        "    if tensors_for_model_forward['p_cont_cov'] is not None: dataset_tensors.append(tensors_for_model_forward['p_cont_cov'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, _LW, 0, dtype=torch.float))\n",
        "\n",
        "    if tensors_for_model_forward['f_cat_cov'] is not None: dataset_tensors.append(tensors_for_model_forward['f_cat_cov'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, _FH, 0, dtype=torch.long))\n",
        "\n",
        "    if tensors_for_model_forward['f_cont_cov'] is not None: dataset_tensors.append(tensors_for_model_forward['f_cont_cov'])\n",
        "    else: dataset_tensors.append(torch.empty(num_samples, _FH, 0, dtype=torch.float))\n",
        "\n",
        "    dataset_tensors.append(y_target_tensor)\n",
        "\n",
        "    if not any(t.shape[0] == num_samples for t in dataset_tensors if t is not None and hasattr(t, 'shape')):\n",
        "        print(f\"Error: Sample count mismatch or no valid tensor. Num_samples: {num_samples}\")\n",
        "        return None\n",
        "\n",
        "    dataset = TensorDataset(*dataset_tensors)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _LW = LOOKBACK_WINDOW\n",
        "    _FH = FORECAST_HORIZON\n",
        "\n",
        "    model_params = {\n",
        "        'target_dim': 1, 'static_categorical_input_dims': [13], 'static_continuous_input_dim': 0,\n",
        "        'obs_categorical_input_dims': [], 'obs_continuous_input_dim': 9,\n",
        "        'known_categorical_input_dims': [12, 31, 7], 'known_continuous_input_dim': 3,\n",
        "    }\n",
        "    num_train_samples = 73761; num_val_samples = 10000\n",
        "\n",
        "    def create_dummy_sequences(num_samples, params, lw, fh):\n",
        "        seq = {}\n",
        "        if params['static_categorical_input_dims']:\n",
        "            seq['x_static_cat'] = np.random.randint(0, params['static_categorical_input_dims'][0], (num_samples, 1)).astype(np.int64)\n",
        "        seq['x_past_target'] = np.random.randn(num_samples, lw, params['target_dim']).astype(np.float32)\n",
        "        if params['obs_continuous_input_dim'] > 0:\n",
        "            seq['x_past_cov_cont'] = np.random.randn(num_samples, lw, params['obs_continuous_input_dim']).astype(np.float32)\n",
        "        else:\n",
        "            seq['x_past_cov_cont'] = np.empty((num_samples, lw, 0), dtype=np.float32)\n",
        "\n",
        "        if params['known_categorical_input_dims']:\n",
        "            fcc_list = []\n",
        "            for card in params['known_categorical_input_dims']:\n",
        "                fcc_list.append(np.random.randint(0, card, (num_samples, fh, 1)).astype(np.int64))\n",
        "            seq['x_future_known_cat'] = np.concatenate(fcc_list, axis=2)\n",
        "        else:\n",
        "            seq['x_future_known_cat'] = np.empty((num_samples, fh, 0), dtype=np.int64)\n",
        "\n",
        "        if params['known_continuous_input_dim'] > 0:\n",
        "            seq['x_future_known_cont'] = np.random.randn(num_samples, fh, params['known_continuous_input_dim']).astype(np.float32)\n",
        "        else:\n",
        "            seq['x_future_known_cont'] = np.empty((num_samples, fh, 0), dtype=np.float32)\n",
        "\n",
        "        seq['y_target'] = np.random.randn(num_samples, fh, params['target_dim']).astype(np.float32)\n",
        "        return seq\n",
        "\n",
        "    train_sequences = create_dummy_sequences(num_train_samples, model_params, _LW, _FH)\n",
        "    val_sequences = create_dummy_sequences(num_val_samples, model_params, _LW, _FH)\n",
        "\n",
        "    dummy_target_data_for_scaler = np.random.randn(100,1)\n",
        "    target_scaler = StandardScaler().fit(dummy_target_data_for_scaler)\n",
        "\n",
        "    d_model_hyperparam = 64\n",
        "    num_heads_hyperparam = 4\n",
        "    lstm_layers_hyperparam = 1\n",
        "    dropout_rate_hyperparam = 0.1\n",
        "    output_quantiles_param = [0.1, 0.5, 0.9]\n",
        "\n",
        "    print(\"--- Starting TFT model instantiation ---\")\n",
        "    tft_model_instance = TemporalFusionTransformer(\n",
        "        target_dim=model_params['target_dim'],\n",
        "        static_categorical_input_dims=model_params['static_categorical_input_dims'],\n",
        "        static_continuous_input_dim=model_params['static_continuous_input_dim'],\n",
        "        obs_categorical_input_dims=model_params['obs_categorical_input_dims'],\n",
        "        obs_continuous_input_dim=model_params['obs_continuous_input_dim'],\n",
        "        known_categorical_input_dims=model_params['known_categorical_input_dims'],\n",
        "        known_continuous_input_dim=model_params['known_continuous_input_dim'],\n",
        "        d_model=d_model_hyperparam, num_attention_heads=num_heads_hyperparam,\n",
        "        lstm_hidden_layers=lstm_layers_hyperparam, dropout_rate=dropout_rate_hyperparam,\n",
        "        lookback_window_size=_LW, forecast_horizon_size=_FH,\n",
        "        output_quantiles=output_quantiles_param\n",
        "    )\n",
        "    print(\"\\nTFT model instance with your parameters created successfully.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tft_model_instance.to(device)\n",
        "\n",
        "    batch_size_training = 256\n",
        "    print(\"\\nCreating DataLoader for training set...\")\n",
        "    train_dataloader = create_dataloader_from_sequences(train_sequences, batch_size_training, model_params, shuffle=True)\n",
        "    print(\"Creating DataLoader for validation set...\")\n",
        "    val_dataloader = create_dataloader_from_sequences(val_sequences, batch_size_training, model_params, shuffle=False)\n",
        "\n",
        "    if train_dataloader is None or val_dataloader is None:\n",
        "        print(\"Error: Training or validation DataLoader not created. Program will stop.\")\n",
        "        exit()\n",
        "\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.0001\n",
        "    optimizer = optim.Adam(tft_model_instance.parameters(), lr=learning_rate)\n",
        "\n",
        "    median_quantile_idx = -1\n",
        "    if 0.5 in output_quantiles_param:\n",
        "            median_quantile_idx = output_quantiles_param.index(0.5)\n",
        "    else:\n",
        "        print(\"Warning: Quantile 0.5 for RMSE calculation is not in the output quantiles list.\")\n",
        "\n",
        "    print(f\"\\n--- Starting training for {num_epochs} epochs on device {device} ---\")\n",
        "    best_val_rmse = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        tft_model_instance.train()\n",
        "        total_train_loss = 0\n",
        "        for batch_idx, batch_tensors_list in enumerate(train_dataloader):\n",
        "            s_cat_b, s_cont_b, p_target_b, p_cat_cov_b, p_cont_cov_b, \\\n",
        "            f_cat_cov_b, f_cont_cov_b, y_true_b = [t.to(device) if t is not None and t.numel() > 0 else None for t in batch_tensors_list]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = tft_model_instance(\n",
        "                s_cat=s_cat_b if model_params['static_categorical_input_dims'] else None,\n",
        "                s_cont=s_cont_b if model_params['static_continuous_input_dim'] > 0 else None,\n",
        "                p_target=p_target_b,\n",
        "                p_cat_cov=p_cat_cov_b if model_params['obs_categorical_input_dims'] else None,\n",
        "                p_cont_cov=p_cont_cov_b if model_params['obs_continuous_input_dim'] > 0 else None,\n",
        "                f_cat_cov=f_cat_cov_b if model_params['known_categorical_input_dims'] else None,\n",
        "                f_cont_cov=f_cont_cov_b if model_params['known_continuous_input_dim'] > 0 else None\n",
        "            )\n",
        "\n",
        "            loss = quantile_loss(predictions, y_true_b, output_quantiles_param)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            if batch_idx > 0 and batch_idx % (len(train_dataloader)//5) == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_dataloader)}, Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1} >> Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        if val_dataloader and median_quantile_idx != -1:\n",
        "            tft_model_instance.eval()\n",
        "            total_val_loss = 0\n",
        "            all_val_preds_median_list = []\n",
        "            all_val_targets_list = []\n",
        "            with torch.no_grad():\n",
        "                for batch_tensors_val in val_dataloader:\n",
        "                    s_cat_v, s_cont_v, p_target_v, p_cat_cov_v, p_cont_cov_v, \\\n",
        "                    f_cat_cov_v, f_cont_cov_v, y_true_v = [t.to(device) if t is not None and t.numel() > 0 else None for t in batch_tensors_val]\n",
        "\n",
        "                    val_predictions = tft_model_instance(\n",
        "                        s_cat_v if model_params['static_categorical_input_dims'] else None,\n",
        "                        s_cont_v if model_params['static_continuous_input_dim'] > 0 else None,\n",
        "                        p_target_v,\n",
        "                        p_cat_cov_v if model_params['obs_categorical_input_dims'] else None,\n",
        "                        p_cont_cov_v if model_params['obs_continuous_input_dim'] > 0 else None,\n",
        "                        f_cat_cov_v if model_params['known_categorical_input_dims'] else None,\n",
        "                        f_cont_cov_v if model_params['known_continuous_input_dim'] > 0 else None\n",
        "                    )\n",
        "                    val_loss = quantile_loss(val_predictions, y_true_v, output_quantiles_param)\n",
        "                    total_val_loss += val_loss.item()\n",
        "\n",
        "                    all_val_preds_median_list.append(val_predictions[:, :, median_quantile_idx].cpu())\n",
        "                    all_val_targets_list.append(y_true_v.cpu())\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "            val_preds_median_all = torch.cat(all_val_preds_median_list, dim=0)\n",
        "            val_targets_all = torch.cat(all_val_targets_list, dim=0)\n",
        "\n",
        "            current_val_rmse = calculate_rmse(val_preds_median_all, val_targets_all, scaler=target_scaler if 'target_scaler' in locals() else None)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} >> Average validation loss: {avg_val_loss:.4f}, Validation RMSE: {current_val_rmse:.4f}\")\n",
        "\n",
        "            if current_val_rmse < best_val_rmse:\n",
        "                best_val_rmse = current_val_rmse\n",
        "                # torch.save(tft_model_instance.state_dict(), \"best_tft_model_SAVES.pth\")\n",
        "                print(f\"*** Better model with RMSE {best_val_rmse:.4f} saved at epoch {epoch+1} (hypothetical). ***\")\n",
        "\n",
        "    print(\"--- Training finished ---\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data loaded. Shape: (105979, 24)\n",
            "Initial columns: ['ایستگاه', 'روزها', 'ژولیوسی شمسی', 'تاریخ شمسی', 'سال شمسی', 'ماه شمسی', 'روز شمسی', 'ژولیوسی میلادی', 'تاریخ میلادی', 'سال میلادی', 'ماه میلادی', 'روز میلادی', 'ماكزيمم دما', 'مينيمم دما', 'ميانگين دما', 'ماكزيمم رطوبت', 'مينيمم رطوبت', 'ميانگين رطوبت', 'بارندگي', 'تبخير', 'ساعات آفتابي', 'سمت باد', 'حداكثر سرعت باد', 'ميانگين فشار QFF']\n",
            "Columns after initial drop (9 columns): ['ایستگاه', 'ژولیوسی شمسی', 'سال شمسی', 'ماه شمسی', 'روز شمسی', 'ماكزيمم دما', 'مينيمم دما', 'ميانگين دما', 'ماكزيمم رطوبت', 'مينيمم رطوبت', 'ميانگين رطوبت', 'بارندگي', 'ساعات آفتابي', 'سمت باد', 'حداكثر سرعت باد']\n",
            "NaN values in column 'سمت باد' were replaced with 0 (before sin/cos).\n",
            "Data shape after removing initial NaNs in critical columns: (105979, 15)\n",
            "Data shape after creating and cleaning derived_day_of_week: (105975, 16)\n",
            "Number of unique stations encoded: 13\n",
            "Column 'سمت باد' was cyclically transformed.\n",
            "Data shape after final sorting and date cleaning: (105975, 20)\n",
            "NaN counts after all filling stages:\n",
            "Series([], dtype: int64)\n",
            "Training set size: (84773, 20), Validation: (15889, 20), Test: (5313, 20)\n",
            "Final list of past continuous covariates for scaling and sequence: ['مينيمم دما', 'ميانگين دما', 'ماكزيمم رطوبت', 'مينيمم رطوبت', 'بارندگي', 'ساعات آفتابي', 'حداكثر سرعت باد', 'سمت باد_sin', 'سمت باد_cos'], Count: 9\n",
            "Columns selected for scaling: ['day_of_year_cos', 'day_of_year_sin', 'بارندگي', 'حداكثر سرعت باد', 'ساعات آفتابي', 'سال شمسی', 'سمت باد_cos', 'سمت باد_sin', 'ماكزيمم دما', 'ماكزيمم رطوبت', 'ميانگين دما', 'مينيمم دما', 'مينيمم رطوبت']\n",
            "\n",
            "Creating sequences for training set...\n",
            "Creating sequences for validation set...\n",
            "Creating sequences for test set...\n",
            "\n",
            "Calculated parameters for TFT model:\n",
            "  target_dim: 1\n",
            "  static_categorical_input_dims: [13]\n",
            "  static_continuous_input_dim: 0\n",
            "  obs_continuous_input_dim: 9\n",
            "  obs_categorical_input_dims: []\n",
            "  known_categorical_input_dims: [12, 31, 7]\n",
            "  known_continuous_input_dim: 3\n",
            "\n",
            "Sample shape of training data (if created):\n",
            "  x_static_cat: (84305, 1)\n",
            "  x_past_target: (84305, 30, 1)\n",
            "  x_past_cov_cont: (84305, 30, 9)\n",
            "  x_future_known_cat: (84305, 7, 3)\n",
            "  x_future_known_cont: (84305, 7, 3)\n",
            "  y_target: (84305, 7, 1)\n",
            "--- Starting TFT model instantiation ---\n",
            "\n",
            "TFT model instance with your parameters created successfully.\n",
            "\n",
            "Creating DataLoader for training set...\n",
            "Creating DataLoader for validation set...\n",
            "\n",
            "--- Starting training for 10 epochs on device cuda ---\n",
            "Epoch 1/10, Batch 57/289, Train Loss: 0.2525\n",
            "Epoch 1/10, Batch 114/289, Train Loss: 0.2530\n",
            "Epoch 1/10, Batch 171/289, Train Loss: 0.2451\n",
            "Epoch 1/10, Batch 228/289, Train Loss: 0.2534\n",
            "Epoch 1/10, Batch 285/289, Train Loss: 0.2532\n",
            "Epoch 1 >> Average training loss: 0.2741\n",
            "Epoch 1 >> Average validation loss: 0.2495, Validation RMSE: 0.9268\n",
            "*** Better model with RMSE 0.9268 saved at epoch 1 (hypothetical). ***\n",
            "Epoch 2/10, Batch 57/289, Train Loss: 0.2491\n",
            "Epoch 2/10, Batch 114/289, Train Loss: 0.2505\n",
            "Epoch 2/10, Batch 171/289, Train Loss: 0.2541\n",
            "Epoch 2/10, Batch 228/289, Train Loss: 0.2461\n",
            "Epoch 2/10, Batch 285/289, Train Loss: 0.2419\n",
            "Epoch 2 >> Average training loss: 0.2503\n",
            "Epoch 2 >> Average validation loss: 0.2492, Validation RMSE: 0.9262\n",
            "*** Better model with RMSE 0.9262 saved at epoch 2 (hypothetical). ***\n",
            "Epoch 3/10, Batch 57/289, Train Loss: 0.2474\n",
            "Epoch 3/10, Batch 114/289, Train Loss: 0.2470\n",
            "Epoch 3/10, Batch 171/289, Train Loss: 0.2561\n",
            "Epoch 3/10, Batch 228/289, Train Loss: 0.2480\n",
            "Epoch 3/10, Batch 285/289, Train Loss: 0.2498\n",
            "Epoch 3 >> Average training loss: 0.2502\n",
            "Epoch 3 >> Average validation loss: 0.2491, Validation RMSE: 0.9262\n",
            "*** Better model with RMSE 0.9262 saved at epoch 3 (hypothetical). ***\n",
            "Epoch 4/10, Batch 57/289, Train Loss: 0.2518\n",
            "Epoch 4/10, Batch 114/289, Train Loss: 0.2535\n",
            "Epoch 4/10, Batch 171/289, Train Loss: 0.2471\n",
            "Epoch 4/10, Batch 228/289, Train Loss: 0.2496\n",
            "Epoch 4/10, Batch 285/289, Train Loss: 0.2553\n",
            "Epoch 4 >> Average training loss: 0.2501\n",
            "Epoch 4 >> Average validation loss: 0.2491, Validation RMSE: 0.9259\n",
            "*** Better model with RMSE 0.9259 saved at epoch 4 (hypothetical). ***\n",
            "Epoch 5/10, Batch 57/289, Train Loss: 0.2532\n",
            "Epoch 5/10, Batch 114/289, Train Loss: 0.2521\n",
            "Epoch 5/10, Batch 171/289, Train Loss: 0.2570\n",
            "Epoch 5/10, Batch 228/289, Train Loss: 0.2485\n",
            "Epoch 5/10, Batch 285/289, Train Loss: 0.2538\n",
            "Epoch 5 >> Average training loss: 0.2500\n",
            "Epoch 5 >> Average validation loss: 0.2490, Validation RMSE: 0.9258\n",
            "*** Better model with RMSE 0.9258 saved at epoch 5 (hypothetical). ***\n",
            "Epoch 6/10, Batch 57/289, Train Loss: 0.2393\n",
            "Epoch 6/10, Batch 114/289, Train Loss: 0.2489\n",
            "Epoch 6/10, Batch 171/289, Train Loss: 0.2451\n",
            "Epoch 6/10, Batch 228/289, Train Loss: 0.2455\n",
            "Epoch 6/10, Batch 285/289, Train Loss: 0.2542\n",
            "Epoch 6 >> Average training loss: 0.2501\n",
            "Epoch 6 >> Average validation loss: 0.2491, Validation RMSE: 0.9260\n",
            "Epoch 7/10, Batch 57/289, Train Loss: 0.2453\n",
            "Epoch 7/10, Batch 114/289, Train Loss: 0.2481\n",
            "Epoch 7/10, Batch 171/289, Train Loss: 0.2582\n",
            "Epoch 7/10, Batch 228/289, Train Loss: 0.2478\n",
            "Epoch 7/10, Batch 285/289, Train Loss: 0.2418\n",
            "Epoch 7 >> Average training loss: 0.2500\n",
            "Epoch 7 >> Average validation loss: 0.2490, Validation RMSE: 0.9260\n",
            "Epoch 8/10, Batch 57/289, Train Loss: 0.2570\n",
            "Epoch 8/10, Batch 114/289, Train Loss: 0.2459\n",
            "Epoch 8/10, Batch 171/289, Train Loss: 0.2538\n",
            "Epoch 8/10, Batch 228/289, Train Loss: 0.2504\n",
            "Epoch 8/10, Batch 285/289, Train Loss: 0.2480\n",
            "Epoch 8 >> Average training loss: 0.2500\n",
            "Epoch 8 >> Average validation loss: 0.2490, Validation RMSE: 0.9258\n",
            "*** Better model with RMSE 0.9258 saved at epoch 8 (hypothetical). ***\n",
            "Epoch 9/10, Batch 57/289, Train Loss: 0.2391\n",
            "Epoch 9/10, Batch 114/289, Train Loss: 0.2447\n",
            "Epoch 9/10, Batch 171/289, Train Loss: 0.2495\n",
            "Epoch 9/10, Batch 228/289, Train Loss: 0.2478\n",
            "Epoch 9/10, Batch 285/289, Train Loss: 0.2491\n",
            "Epoch 9 >> Average training loss: 0.2500\n",
            "Epoch 9 >> Average validation loss: 0.2490, Validation RMSE: 0.9258\n",
            "*** Better model with RMSE 0.9258 saved at epoch 9 (hypothetical). ***\n",
            "Epoch 10/10, Batch 57/289, Train Loss: 0.2565\n",
            "Epoch 10/10, Batch 114/289, Train Loss: 0.2534\n",
            "Epoch 10/10, Batch 171/289, Train Loss: 0.2555\n",
            "Epoch 10/10, Batch 228/289, Train Loss: 0.2539\n",
            "Epoch 10/10, Batch 285/289, Train Loss: 0.2520\n",
            "Epoch 10 >> Average training loss: 0.2500\n",
            "Epoch 10 >> Average validation loss: 0.2491, Validation RMSE: 0.9262\n",
            "--- Training finished ---\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7s7jRBlF1F5",
        "outputId": "720f0a07-5c04-4f2c-8460-660bc25982f1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}